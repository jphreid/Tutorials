{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MILA_rnn_lstm_solution.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6pL7OrdnLVoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tutoriel sur les réseaux neuronaux récurrents\n",
        "\n",
        "### jean.philippe.reid@rd.mila.quebec\n",
        "\n",
        "### Mars 2018\n"
      ]
    },
    {
      "metadata": {
        "id": "sftgX3Cu0v-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Références\n",
        "\n",
        "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "https://distill.pub/2016/augmented-rnns/\n"
      ]
    },
    {
      "metadata": {
        "id": "V_tMgB2wTQTF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Module utilitaire"
      ]
    },
    {
      "metadata": {
        "id": "OBgc1wsCtAGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Premièrement, installons pytorch et certains modules nécessaires pour compléter ce tutoriel. "
      ]
    },
    {
      "metadata": {
        "id": "kgw2adqtQHAe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Ces quelques lignes de code doivent être considérées seulement si \n",
        "# vous utilisez l'environnement COLAB. \n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iF-E5xOQRe_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.utils.data as data_utils\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "torch.backends.cudnn.version()\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cuda = False\n",
        "\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WXqWpBdaQm0-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def normalized(x, y):\n",
        "  '''Cette fonction normalize un le tenseur 'x'. '''\n",
        "  x = (x - torch.min(x,0)[0].view(1,x.shape[1])) / ((torch.max(x,0)[0] - torch.min(x,0)[0]).view(1,x.shape[1]))\n",
        "\n",
        "  return x, x.sum(dim=1)\n",
        "\n",
        "def standardized(x, y):\n",
        "  '''  Cette fonction standardize un le tenseur 'x'. '''  \n",
        "  x = (x - torch.mean(x, 0).view(1,x.shape[1])) / torch.std(x, 0).view(1,x.shape[1])\n",
        "  \n",
        "  return x, x.sum(dim=1)\n",
        "\n",
        "def data_set(N, T, interval):\n",
        "    ''' \n",
        "    inputs : \n",
        "    N : nombre de données \n",
        "    T : longueur de la séquence\n",
        "    interval : l'intervalle dans lequel les nombres seront tirés pour générer les séquences. \n",
        "    \n",
        "    outputs: retourne les séquences (xx) et les cibles (yy), en format torch.tensor \n",
        "    '''\n",
        "    \n",
        "    x = (torch.Tensor(N, T).random_(0, 2 * interval[1]) + interval[0])\n",
        "   \n",
        "    return x, x.sum(dim=1)\n",
        "  \n",
        "def print_sequence(x, y):\n",
        "    '''\n",
        "    x : Une séquence particulière, i.e dim(x) = 1 x T\n",
        "    y : Cible liée à cette même séquence, i.e. dim(y) = 1\n",
        "    retourne une série de caractères illustrant la séquence \n",
        "    dans un format convivial. \n",
        "    '''\n",
        "    n = x.shape[0]\n",
        "    for i, x_ in enumerate(x):\n",
        "        if i == 0 : \n",
        "            string=' ' + str(x_)\n",
        "        else: \n",
        "            string=string + ' + '+str(x_)\n",
        "\n",
        "    return string+' = ' + str(y)\n",
        "  \n",
        "def adjust_lr(optimizer, lr0, epoch, total_epochs):\n",
        "    '''\n",
        "    Cette fonction diminue le taux d'apprentissage suivant une fonction \n",
        "    exponentielle avec le nombre d'époque. \n",
        "    \n",
        "    optimizer: e.g. optim.SGD(... )\n",
        "    lr0 : taux d'apprentissage initial\n",
        "    epoch : époque à laquelle la mise à jour est effectuée. \n",
        "    total epochs: nombre d'époque totale\n",
        "    \n",
        "    exemple : \n",
        "    \n",
        "    new_learning_rate = adjust_lr(optimizer, 0.01, e_, 100)\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    lr = lr0 * (0.36 ** (epoch / float(total_epochs)))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "def batch_loss(data_batch,model): \n",
        "    '''\n",
        "    Cette fonction calcule le loss d'un jeu de donnée divisé en plusieurs   \n",
        "    batchs. C'est nécessaire pour traiter de grandes quantités de données.     \n",
        "    '''\n",
        "    loss = 0\n",
        "    n = 0\n",
        "    \n",
        "    for batch_idx, (x, y) in enumerate(data_batch):\n",
        "        x, y = model.input_format(x, y)\n",
        "        out = model(x)\n",
        "        loss += model.criterion(out, y)\n",
        "        n += 1\n",
        "    return loss.data[0] / n\n",
        "  \n",
        "  \n",
        "def adjust_fontsize(ax):\n",
        "  '''\n",
        "  Par déformation professionnelle, j'ai le souci de préparer de belles figures. \n",
        "  Cette fonction est un clin d'oeil pour mon ancien superviseur de thèse.\n",
        "  '''\n",
        "  for ax in ax:\n",
        "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n",
        "                 + ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "      item.set_fontsize(14)  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51tYmwJ2qoqU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Objectif"
      ]
    },
    {
      "metadata": {
        "id": "ZDMnGX4iqoqV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "L'objectif de ce tutoriel est de construire un modèle capable d'additionner ou soustraire une série de nombres. Ce jeu de données est facile à générer et nous permet de tester la capacité de plusieurs algorithmes tels que,\n",
        "\n",
        "* RNN, LSTM \n",
        "* MLP\n"
      ]
    },
    {
      "metadata": {
        "id": "jS8xrTCXqoqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Le jeu de données est constitué d'une séquence de nombre de longueurs *seq_len* , à laquelle une cible est associée. Dans l'exemple ci-dessous, la i$^{eme}$ composante des données est explicitement détaillée (voir la Fig. 1). \n",
        "\n",
        "\\begin{align}  \n",
        "\\mathrm x^{(i)} &= \\left[ 4,-1,15,24\\right], \\mathrm x^{(i)} \\in \\mathbb R^{d_0} \\\\ \n",
        "\\mathrm y^{(i)} &= 42, \\mathrm y^{(i)} \\in \\mathbb R \n",
        "\\end{align}\n",
        "\n",
        "Il est important de noter que chaque composante du vecteur $\\mathrm x^{(i)}$, peut être de plusieurs dimensions, c'est-à-dire $x^{(i)}_j \\in \\mathbb R^{d_1}$ où $d_1 > 1$. \n"
      ]
    },
    {
      "metadata": {
        "id": "QzmAxECSqoqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ce jeu de donné peut ainsi servir à entrainer un modèle de réseau de neurones récurrents (RNN), tel que le _long short term memory_(LSTM). Dans ce cas, chaque nombre sera l'entrée d'une couche cachée (_hidden layer_) de dimension $h_d$. \n",
        "\n",
        "Comme la cible est un nombre réel, il est nécessaire de rajouter une couche linéaire au modèle pour \"ajuster\" les dimensions (voir la Fig.2). Cette notion sera expliquée à l'aide d'un exemple détaillé ci-bas. "
      ]
    },
    {
      "metadata": {
        "id": "qDCPei0QqoqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.001.jpeg)\n",
        "\n",
        "Fig.1 : Jeu de données considéré. "
      ]
    },
    {
      "metadata": {
        "id": "ok4sQtHUqoqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.002.jpeg)\n",
        "\n",
        "\n",
        "Fig.2 : Schéma d'un réseau récurrent. "
      ]
    },
    {
      "metadata": {
        "id": "RCnneiTsqoqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Réseau de neuronnes récurrents (LSTM)"
      ]
    },
    {
      "metadata": {
        "id": "IAisYYriqoqZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Générer une couche LSTM\n",
        "###  __ = nn.LSTM(input_size, hidden_size, num_layers)"
      ]
    },
    {
      "metadata": {
        "id": "u93VR8oVqoqa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Une couche LSTM peut incorporer plusieurs paramètres dont certains régis par le jeu de données (*input_size*), mais aussi d'autres paramètres essentiels pour optimiser la capacité du modèle, tels que *hidden_size*, *num_layers*, etc."
      ]
    },
    {
      "metadata": {
        "id": "FgCALpkgqoqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Entrés d'une couche LSTM (*inputs*)\n",
        "### __ = <font color='red'>LSTM</font>(*input*, (h0,c0))"
      ]
    },
    {
      "metadata": {
        "id": "EXaWaGleqoqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "En plus des données (_input_), il est aussi possible d'initialiser les tenseurs h_0, c_0 définis comme les *hidden* et *cell states*,  paramètres essentiels aux LSTMs. \n",
        "\n",
        "__Dans le cas où $h_0$ et $c_0$ ne sont pas définis, le module LSTM utilisera les valeurs par défaut, i.e. 0.__"
      ]
    },
    {
      "metadata": {
        "id": "JrRFyn_Tqoqc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Donnée d'entrée (*input*)\n",
        "__<font color='red'>input</font> =  torch.Tensor(seq_len, batch_size, input_size) __"
      ]
    },
    {
      "metadata": {
        "id": "RKuc_SyMqoqd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il est nécessaire de réorganiser les données d'entrées (input) selon trois paramètres : \n",
        "\n",
        "* la longueur de la séquence (seq_len)\n",
        "* la grandeur du lot (batch, i.e. batch_size)\n",
        "* les dimensions des entrées (input_size)\n",
        "\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "Dh9ca_HMiKuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Donnée de sortie (*output*)\n",
        "### output = LSTM(*input*, (h0,c0))\n",
        "__<font color='red'>output</font> = torch.tensor(seq_len,batch_size, hidden_size x num_directions) __\n"
      ]
    },
    {
      "metadata": {
        "id": "HGShtHdArgvw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Les dimensions du tenseur \"output_0\" sont déterminées par: (*seq_len*, *batch*, *hidden_size* $\\times$ *num_directions*). \n",
        "\n",
        "Dans le cas qui nous intéresse, num_directions $ = 1$. "
      ]
    },
    {
      "metadata": {
        "id": "zvtzfm1qaj92",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.003.jpeg)\n",
        "\n",
        "Fig. 3: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
      ]
    },
    {
      "metadata": {
        "id": "9P1NDwZqqoqc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.004.jpeg)\n",
        "\n",
        "Fig. 4: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
      ]
    },
    {
      "metadata": {
        "id": "n-bi9ydcqoqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.005.jpeg)\n",
        "\n",
        "Fig.5 : http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
      ]
    },
    {
      "metadata": {
        "id": "Maj8vgFlyQlo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exemple"
      ]
    },
    {
      "metadata": {
        "id": "qOdqVEOp9Zbz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Créons un jeu de données $\\bf x$ composés de 100 séquences de 4 nombres entre 0 et 100. Les cibles $\\bf y$ correspondent à la somme de chacune de ces séquences. La fonction *data_set* située dans la section *Module utilitaire* s'occupe de cette tâche fastidieuse. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "14lZPQwTQek8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Création d'un jeu de donnés \n",
        "x,y = data_set(100, 4, [-100, 100])\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4YwIajCYJ5vx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sequence = print_sequence(x[1,:] ,y[1])\n",
        "\n",
        "print(sequence)\n",
        "print('Dimensions des entrées : {} x {}'.format(*x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jTKwBXDmqoqk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Q1__ : Selon l'exemple ci-haut, quels sont les paramètres d'entrées? On considéra ici qu'un seul lot (*batch*). \n",
        "\n",
        "* batch_size = \n",
        "* seq_len = \n",
        "* input = \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "poK1aylHJ53Q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# RÉPONSE: \n",
        "x_ = x[:,:,np.newaxis]\n",
        "y_ = y[:,np.newaxis].type(torch.FloatTensor)\n",
        "\n",
        "# ...\n",
        "\n",
        "print('Dimensions du tenseur x = {}'.format(x_.shape))\n",
        "print('______________________________________________')\n",
        "print('    ')\n",
        "\n",
        "print('Plus en détails: ')\n",
        "print('_________________')\n",
        "print('    ')\n",
        "\n",
        "dimensions = x_.shape\n",
        "print('batch_size = {}   seq_len = {}     input_size = {}'.format(*dimensions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAJ9tNujqoql",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Q2__  : Donnez un exemple où les dimensions d'entrées sont supérieures à un, c.-à-d. *input* $> 1$.\n",
        "\n",
        "Réponses : \n",
        "* chaque mot d'une phrase est représenté par un vecteur de dimension égale à la grandeur du vocabulaire. \n",
        "* température réelle, vent, température ressentie vs. t\n",
        "* variation boursière "
      ]
    },
    {
      "metadata": {
        "id": "z7HNL-qomf35",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.006.jpeg)\n",
        "\n",
        "\n",
        "Fig.6 : Exemple où la dimension des entrées est supérieure à un. "
      ]
    },
    {
      "metadata": {
        "id": "2Y4AW07Jqoqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Q3__  : Construisez votre première couche LSTM. Nous vous référons au lien dessous la Fig. 3 pour plus de détails et exemples. "
      ]
    },
    {
      "metadata": {
        "id": "0zyJ2FFBJ57d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "# les paramètres\n",
        "input_size = 1\n",
        "hidden_size = 6\n",
        "num_layers = 1\n",
        "\n",
        "# lstm  \n",
        "lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
        "\n",
        "# forward \n",
        "output_0, (hn, cn) = lstm(Variable(x_))\n",
        "\n",
        "print('Dimensions - output_0: {}'.format(output_0.shape))\n",
        "print('Dimensions - h_n: {}'.format(hn.shape))\n",
        "print('Dimensions - c_n: {}'.format(cn.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MX0t3nT98kAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Réorganiser les dimensions de la sortie (\"<font color='red'>output_0</font>\") "
      ]
    },
    {
      "metadata": {
        "id": "ZIVvInzN8zqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tel que mentionné ci-haut, seule la dernière composante du tenseur \"output_0\" sera considérée pour prédire la cible associée à une séquence (voir Fig.2). \n",
        "\n",
        "Exemple: considérons la $41^e$ séquence ainsi que la couche associée. "
      ]
    },
    {
      "metadata": {
        "id": "w50E_nbBJ5-e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(x_[41,:,:])\n",
        "print(y_[41])\n",
        "\n",
        "sequence = print_sequence(x[41,:],y[41])\n",
        "print(sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F9ZExe-iJ6BE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(output_0[41,:,:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vat-3CyN9j9y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tel que mentionné ci-haut, seule la quatrième (dernière) composante du tenseur output_0 qui doit être considéré (voir Fig. 2)."
      ]
    },
    {
      "metadata": {
        "id": "Jt5sipkq97_b",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "output_0[41,-1,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDD1ICcJL_B7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Le tenseur $h_n$ regroupe la dernière sortie (*output*) de chaque séquence."
      ]
    },
    {
      "metadata": {
        "id": "7rh_45NWMbqT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(hn[:,41,:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ewppj5yvsVJT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Q4__  : Proposez une stratégie pour transformer le tenseur \"output_0\" de dimensions [100,4,6] à un tenseur de dimensions voulues, c.-à-d. [100,1]. Spécifiquement, quelle opération mathématique permettra d'obtenir les dimensions voulues? \n",
        "    * Réponse : Effectuer un produit tensoriel entre output_0 et une tenseur \"M\" dont le résultat est de dimensions voulues, c.-à-d. 100 x 1\n",
        "\n",
        "\\begin{align}\n",
        "\\text{output} =  \\text{output_0}\\;  M \n",
        "\\end{align}\n",
        "Dans le cas qui nous intéresse, les dimensions du tenseur \"M\" sont de $1 \\times$ *hidden_size* \n"
      ]
    },
    {
      "metadata": {
        "id": "BaS5Ku1GJ6Dr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## Question\n",
        "\n",
        "'''\n",
        "Première partie: générez un tenseur M de dimensions voulues et utilisez \n",
        "la fonction 'torch.matmul' pour  multiplier M et output_0. \n",
        "\n",
        "Indice : Le tenseur M doit être une 'Variable' pour être multiplié au tenseur \n",
        "output_0\n",
        "\n",
        "M = Variable(torch.Tensor( _?_, _?_ ))\n",
        "output = torch.matmul(output_0}[_?_, _?_, _?_], M)\n",
        "\n",
        "'''\n",
        "M = Variable(torch.Tensor(hidden_size, 1))\n",
        "output = torch.matmul(output_0[:,-1,:], M)\n",
        "\n",
        "'''\n",
        "Deuxième partie: gérérez une couche linéaire de dimensions voulues avec la \n",
        "fonction 'LL = nn.Linear(_?_, _?_, _?_)'. \n",
        "\n",
        "L'entrée de LL sera output_0, i.e. output = LL(output_0[_?_, _?_, _?_])\n",
        "''' \n",
        "\n",
        "LL = nn.Linear(hidden_size, input_size,bias=True)\n",
        "output = LL(output_0[:,-1,:])\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dnWwn0qbAjQG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Construction du modèle"
      ]
    },
    {
      "metadata": {
        "id": "af7055DXtEq9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il est fortement suggéré d'utiliser des classes pour définir les modèles dans l'environnement Pytorch, et ce peu importe son architecture. \n"
      ]
    },
    {
      "metadata": {
        "id": "zjTN6N9pB96s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Q5__  : Écrivez la fonction RnnLinear.forward(). Cette fonction aura comme entrée *self* et le tenseur $\\bf x$ de dimension N x T x 1. \n",
        "\n",
        "Indice : le output_0 de la couche LSTM ou RNN sera l'entrée de la couche linéaire. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YRLdrWvNJ6Kz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class RnnLinear(nn.Module):\n",
        "    def __init__(self,input_size, hidden_size, num_layers, lstm_or_rnn,cuda):\n",
        "        '''\n",
        "        Les paramètres importants du modèle sont définies dans cette fonction. \n",
        "        '''\n",
        "        num_directions = 1\n",
        "        super(RnnLinear, self).__init__()\n",
        "        \n",
        "        # parametres importants \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm_or_rnn = lstm_or_rnn      \n",
        "        \n",
        "        # couches de notre réseau \n",
        "        if lstm_or_rnn == 'lstm':\n",
        "            self.lstm = nn.LSTM(input_size, hidden_size, \n",
        "                                num_layers,dropout = 0.5,\n",
        "                                batch_first = True)\n",
        "            if cuda : self.lstm.cuda() \n",
        "              \n",
        "        elif lstm_or_rnn == 'rnn':\n",
        "            self.rnn = nn.RNN(input_size, hidden_size,\n",
        "                              num_layers,dropout = 0.5, \n",
        "                              batch_first = True\n",
        "                             )\n",
        "            if cuda: self.rnn.cuda()\n",
        "              \n",
        "        else: print('You made a mistake, pal!')\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, input_size,bias=True)\n",
        "        if cuda: \n",
        "          self.linear.cuda()\n",
        "              \n",
        "        # criteria for SGD\n",
        "        self.criterion = nn.MSELoss()\n",
        "        \n",
        "    def input_format(self,x,y):\n",
        "        '''\n",
        "        Cette fonction permet de préparer les dimensions des données d'entrées. \n",
        "        '''\n",
        "        x = x[:,:,None]\n",
        "        y = y[:,None].type(torch.FloatTensor)\n",
        "        \n",
        "        if cuda : \n",
        "            x = Variable(x.cuda())\n",
        "            y = Variable(y.cuda())\n",
        "            \n",
        "        else: \n",
        "            x = Variable(x)\n",
        "            y = Variable(y)\n",
        "        return x,y\n",
        "\n",
        "    def grad_norm(self):\n",
        "        total_norm=0\n",
        "        for p in list(self.parameters()):\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm\n",
        "\n",
        "        return total_norm\n",
        "  \n",
        "    def forward(self,x):\n",
        "        '''\n",
        "        Cette fonction devrait inclure des conditions if/elif sur la variable \n",
        "        self.lstm_or_rnn. La sortie des couches LSTM et RNN seront les entrées \n",
        "        de la couche linéaire. \n",
        "        '''\n",
        "        \n",
        "        # cas où self.lstm_or_rnn = 'rnn'\n",
        "        if self.lstm_or_rnn == 'rnn':\n",
        "            out1,(__) = self.rnn(x)\n",
        "\n",
        "        # cas où self.lstm_or_rnn = 'lstm'      \n",
        "        if self.lstm_or_rnn == 'lstm':\n",
        "            out1,(__) = self.lstm(x)\n",
        "\n",
        "        # votre couche linéraire             \n",
        "        out2 = self.linear(out1[:,-1,:])\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFBy2S82DCS2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Testons notre modèle!\n",
        "\n",
        "Prenez le temps de bien comprendre les dimensions affichées ci-bas. "
      ]
    },
    {
      "metadata": {
        "id": "ee-kXlLaDKKV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x,y = data_set(100, 4, [0,100])\n",
        "\n",
        "\n",
        "input_size = 1\n",
        "num_layers = 2\n",
        "hidden_size = 40 \n",
        "\n",
        "model = RnnLinear(input_size, hidden_size, num_layers, 'lstm', cuda)\n",
        "\n",
        "xx,yy = model.input_format(x, y)\n",
        "print('Dimensions initiales des données : {}'.format(x.shape))\n",
        "print('Dimensions des données formatées : {}'.format(xx.shape))\n",
        "\n",
        "pred = model(xx)\n",
        "print('Dimensions des prédictions : {}'.format(pred.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SMsFy8V1B0vk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "__Q6__\n",
        "\n",
        "Construisez la boucle d'entrainement ci-bas. \n",
        "\n",
        "__Q7__\n",
        "\n",
        "Entrainez le modèle sur 20 époques avec ces paramètres et hyperparamètres, c.-à-dire $N_{train}$ = 20000, T = 10, intervalle = [0,1000], $h_d$ = 20, num_layers = 2, lr0 = 0.05. \n",
        "\n",
        "__Q8__ : \n",
        "\n",
        "Testez le modèle sur l'ensemble de test. Commentez les résultats obtenus.  \n",
        "\n",
        "Note: L1 loss est définie comme la somme des différences absolue entre les cibles et les prédictions. Pour l'ensemble de test considéré, cette quantité est définie comme \\begin{align}\n",
        "L1 = \\sum_{i=1}^{5000} \\mid y^{(i)} - \\text{output}^{(i)}\\mid \\end{align}\n",
        "\n",
        "__Q9__ \n",
        "\n",
        "Augmentez la capacité du modèle, en fixant $h_d$ = 40. Comparez les résultats à ceux précédents.\n",
        "\n",
        "__Q10__ \n",
        "\n",
        "Finalement, *'standardisez'* les données de chaque séquence. Vous devrez utiliser la fonction *x, y = standardized(x, y)* définie dans le module utilitaire. \n",
        "\n",
        "De plus, élargissez l'intervalle de tirage de -1000 à 1000 et entrainez jusqu'à 100 époques. Comparez les résultats à ceux précédents.\n",
        "\n",
        "__Q11__\n",
        "\n",
        "Il est possible d'activer ou de désactiver l'option dropout de notre modèle en appelant 'model.train()' et 'model.eval()' respectivement. Testez ces deux modes. \n",
        "\n",
        "Pourquoi est-il nécessaire de désactiver l'option 'dropout' lors des phases test et validation? \n",
        "\n",
        "__Q12__ \n",
        "\n",
        "Explorez différents hyperparamètres. \n",
        "\n",
        "* lr = 0.1, 1, 0.00001\n",
        "\n",
        "* hidden_size = 100,150, 200\n",
        "\n",
        "* dropout = 0.1, 0.3, 0.5\n",
        "\n",
        "Enjoy! \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tnV7bUgZsvz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Création du jeu de données"
      ]
    },
    {
      "metadata": {
        "id": "c8AsbJZmJ6OB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x,y = data_set(20000, 10, [0,1000])\n",
        "\n",
        "#  standardized\n",
        "xtrain, ytrain = (x[:10000], y[:10000])\n",
        "xvalid, yvalid = (x[10000:15000], y[10000:15000])\n",
        "xtest, ytest = (x[15000:], y[15000:])\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "all_data_train = data_utils.DataLoader(data_utils.TensorDataset(xtrain, ytrain),\n",
        "                                       batch_size, shuffle=True)\n",
        "all_data_valid = data_utils.DataLoader(data_utils.TensorDataset(xvalid, yvalid), \n",
        "                                       batch_size, shuffle=False)\n",
        "all_data_test = data_utils.DataLoader(data_utils.TensorDataset(xtest, ytest),\n",
        "                                       batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxVLLPwMFh0o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Entraînement du modèle"
      ]
    },
    {
      "metadata": {
        "id": "srcOUXeqQrZl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "num_layers = 2\n",
        "hidden_size = 20\n",
        "number_epoch = 20\n",
        "lr0 = 0.05\n",
        "\n",
        "model = RnnLinear(input_size, hidden_size, num_layers,'lstm',cuda)\n",
        "optimizer = optim.SGD(model.parameters(),lr=lr0)\n",
        "\n",
        "\n",
        "t0 = time.clock()\n",
        "\n",
        "# tableau à remplir lors de la phase d'entrainement \n",
        "loss_train_data = np.array([]) \n",
        "loss_valid_data = np.array([]) \n",
        "\n",
        "total_norm = np.array([])\n",
        "\n",
        "t0 = time.clock()  \n",
        "\n",
        "for e_ in range(number_epoch):\n",
        "  \n",
        "  # model.train(), poruquoi? Indice : dropout   \n",
        "  model.train()\n",
        "  for batch_idx, (xx, yy) in enumerate(all_data_train):                \n",
        "    # Formatez les données\n",
        "    xx,yy = model.input_format(xx,yy)\n",
        "    \n",
        "    # Évaluez le modèle \n",
        "    pred_batch = model(xx)\n",
        "    \n",
        "    # Initialisez les gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Calculez la loss  \n",
        "    loss_batch = model.criterion(pred_batch,yy)\n",
        "    \n",
        "    # SGD backward \n",
        "    loss_batch.backward()\n",
        "    \n",
        "    # SGD optimized\n",
        "    optimizer.step()\n",
        "    \n",
        "    # stop. \n",
        "  \n",
        "  # Calculez la norm de tous les gradients \n",
        "  total_norm = np.append(total_norm,model.grad_norm())\n",
        "\n",
        "  # model.eval(), pourquoi? Indice : dropout \n",
        "  model.eval()  \n",
        "\n",
        "  loss_train_data = np.append(loss_train_data, batch_loss(all_data_train,model)) \n",
        "  loss_valid_data = np.append(loss_valid_data, batch_loss(all_data_valid,model))\n",
        "\n",
        "  if e_%10 == 0: \n",
        "    # '{} {}'.format('foo', 'bar')\n",
        "    print('N. of Epochs = {}, Train Loss = {}, Valid Loss = {}'.format(e_, loss_train_data[e_], loss_valid_data[e_]))\n",
        "\n",
        "\n",
        "  lr_ = adjust_lr(optimizer, lr0, e_, number_epoch)\n",
        "\n",
        "tf = time.clock()\n",
        "print('Terminé, {} sec'.format(tf-t0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16ZBNnYKFJCS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prédiction du modèle"
      ]
    },
    {
      "metadata": {
        "id": "_r19roAVGeSk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# prêt pour évaluer\n",
        "model.eval()\n",
        "\n",
        "# prêt pour entraîner\n",
        "# model.train()\n",
        "\n",
        "xtest_,ytest_ = model.input_format(xtest, ytest)\n",
        "pred_test_all = model(xtest_)\n",
        "\n",
        "L1_loss = torch.sum(torch.abs(pred_test_all - ytest_)).data[0]\n",
        "\n",
        "print('LSTM L1 loss = {}'.format(L1_loss))\n",
        "print('      ')\n",
        "print('Prediction')\n",
        "print('___________________')\n",
        "print(pred_test_all[0:10])\n",
        "print('      ')\n",
        "print('Target')\n",
        "print('___________________')\n",
        "print(ytest_[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7yLEDkGpD_Zj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfQB8XPdF8wO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Courbes d'apprentissage"
      ]
    },
    {
      "metadata": {
        "id": "zKcZTByAxCjk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nous traçons plus bas deux graphiques d'importance capitale : \n",
        "\n",
        "* Courbe d'apprentissage, i.e. loss vs. époque pour l'ensemble de validation et d'entrainement. \n",
        "* La norme de tous les gradients évaluée à chaque époque. \n"
      ]
    },
    {
      "metadata": {
        "id": "dygg3CwaKwxw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,6))\n",
        "ax1.plot(loss_train_data,'-k',label='Train')\n",
        "ax1.plot(loss_valid_data,'-r',label='Valid')\n",
        "\n",
        "ax2.plot(total_norm,'-b',label='Grad')\n",
        "\n",
        "\n",
        "ax1.set_xlabel('epoch',fontsize=14)\n",
        "ax2.set_xlabel('epoch',fontsize=14)\n",
        "ax1.set_ylabel('MSE loss',fontsize=14)\t\n",
        "ax2.set_ylabel('Grad Norm',fontsize=14)\t\n",
        "legend = ax1.legend(loc='upper right',fontsize=14)\n",
        "\n",
        "ax1.set_xlim((0))\n",
        "\n",
        "ax1.set_title('Learning curve', fontsize = 14)\n",
        "ax1.set_title('Learning curve', fontsize = 14)\n",
        "\n",
        "xmin, xmax = ax2.get_xlim()\n",
        "ymin, ymax = ax2.get_ylim()\n",
        "\n",
        "ax2.text(0.6*(xmax - xmin) + xmin, 0.8*(ymax-ymin) + ymin,\n",
        "         'num_layers : '+str(num_layers), fontsize=14)\n",
        "\n",
        "ax2.text(0.6*(xmax - xmin) + xmin, 0.72*(ymax-ymin) + ymin,\n",
        "         'hidden_size : '+str(hidden_size), fontsize=14)\n",
        "\n",
        "ax2.text(0.6*(xmax - xmin) + xmin, 0.64*(ymax-ymin) + ymin,\n",
        "         'number_epoch : '+str(number_epoch), fontsize=14)\n",
        "\n",
        "ax2.text(0.6*(xmax - xmin) + xmin, 0.56*(ymax-ymin) + ymin,\n",
        "         'lr$_0$ : '+str(lr0), fontsize=14)\n",
        "\n",
        "\n",
        "adjust_fontsize([ax1,ax2])\n",
        "plt.show()          \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uXLLEtnrGBdn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question\n",
        "\n",
        "__Q13__ : Dans le graphique ci-haut, comment expliquez-vous que la norme diminue et converge vers zéro avec le nombre d'époques? \n",
        "\n",
        "..."
      ]
    },
    {
      "metadata": {
        "id": "iqPYtKa0E4UR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prédiction sur des séquences de longueurs différentes"
      ]
    },
    {
      "metadata": {
        "id": "-vKX2NMyF5oj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question"
      ]
    },
    {
      "metadata": {
        "id": "EA9MAlcqaNae",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Q14__ : Prouvez qu'il est aussi possible d'utiliser le modèle entrainé pour prédire des séquences d'une longueur différente."
      ]
    },
    {
      "metadata": {
        "id": "xr5OPW0W7lnd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "T_= 12\n",
        "# xtrain, ytrain = normalized(x[:20000], y[:20000])\n",
        "\n",
        "xx, yy = data_set(500,T_,[0,100])\n",
        "xx, yy = normalized(xx, yy)\n",
        "\n",
        "xx, yy = model.input_format(xx, yy)\n",
        "\n",
        "print('Séquence de longueur {}'.format(T_))\n",
        "print('--------------------------')\n",
        "print('Prédiction: {}'.format(model(xx).data[0][0]))\n",
        "\n",
        "print('Cible: {}'.format(yy.data[0][0]))\n",
        "print('LSTM L1 loss: {}'.format((torch.abs(model(xx).data[0]-yy.data[0])[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJ1XwmGVKGVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Comparaison entre RNN et LSTM"
      ]
    },
    {
      "metadata": {
        "id": "SvbYU2T1KVwk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YMXnblC77hwv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "__Q15__ : Entrainez le modèle ci-bas pour ces séries d'hyperparamètres et comparez les performances des modèles RNN et LSTM. \n",
        "\n",
        "* 'Stardardized' les données. \n",
        "* N = 30000\n",
        "* $h_d$ = 50\n",
        "* number_epoch = 100\n",
        "* batch_size = 100\n",
        "* lr0 = 0.005\n",
        "\n",
        "  * T = 20, 50, 100, 200\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BXM6MKtmKw7v",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "cuda = True \n",
        "\n",
        "input_size = 1\n",
        "num_layers = 2\n",
        "hidden_size = 50\n",
        "number_epoch = 100\n",
        "batch_size = 100\n",
        "lr0 = 0.005\n",
        "\n",
        "loss_train_data_rnn = np.array([])\n",
        "loss_train_data_lstm = np.array([])\n",
        "\n",
        "loss_valid_data_rnn = np.array([])\n",
        "loss_valid_data_lstm = np.array([])\n",
        "\n",
        "total_norm_rnn = np.array([])\n",
        "total_norm_lstm = np.array([])\n",
        "\n",
        "# Génération des données\n",
        "T = 100\n",
        "\n",
        "x,y = data_set(30000, T, [-10000,10000])\n",
        "\n",
        "xtrain, ytrain = normalized(x[:20000], y[:20000])\n",
        "xvalid, yvalid = normalized(x[20000:25000], y[20000:25000])\n",
        "xtest, ytest = normalized(x[25000:], y[25000:])\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "all_data_train = data_utils.DataLoader(data_utils.TensorDataset\n",
        "                          (xtrain, ytrain),batch_size, shuffle=True)\n",
        "  \n",
        "all_data_valid = data_utils.DataLoader(data_utils.TensorDataset\n",
        "                          (xvalid, yvalid),batch_size, shuffle=False)\n",
        "\n",
        "all_data_test = data_utils.DataLoader(data_utils.TensorDataset\n",
        "                          (xtest, ytest),batch_size, shuffle=False)\n",
        "\n",
        "# Initialisation des modèeles\n",
        "model_rnn = RnnLinear(input_size, hidden_size, num_layers, 'rnn', cuda)\n",
        "model_lstm = RnnLinear(input_size, hidden_size, num_layers, 'lstm', cuda)\n",
        "optimizer_rnn = optim.SGD(model_rnn.parameters(), lr=lr0)\n",
        "optimizer_lstm = optim.SGD(model_lstm.parameters(), lr=lr0)\n",
        "\n",
        "t0 = time.clock()  \n",
        "for e_ in range(number_epoch):  \n",
        "\n",
        "  model_rnn.train()\n",
        "    \n",
        "  ##################\n",
        "  ###### RNN ######\n",
        "  ##################\n",
        "    \n",
        "  for batch_idx, (xx, yy) in enumerate(all_data_train):                \n",
        "    xx, yy = model_rnn.input_format(xx, yy)\n",
        "    pred_batch_rnn = model_rnn(xx)\n",
        "    optimizer_rnn.zero_grad()\n",
        "    loss_batch_rnn = model_rnn.criterion(pred_batch_rnn, yy)\n",
        "    loss_batch_rnn.backward()\n",
        "#     torch.nn.utils.clip_grad_norm(model_rnn.parameters(), 0.2, norm_type=2)\n",
        "    optimizer_rnn.step()\n",
        "\n",
        "  model_rnn.eval()\n",
        "  lt = batch_loss(all_data_train, model_rnn)\n",
        "  lv = batch_loss(all_data_valid, model_rnn)\n",
        "  loss_train_data_rnn = np.append(loss_train_data_rnn, lt)\n",
        "  loss_valid_data_rnn = np.append(loss_valid_data_rnn, lv)\n",
        "    \n",
        "  total_norm_rnn = np.append(total_norm_rnn,model_rnn.grad_norm())\n",
        "    \n",
        "  ##################\n",
        "  ###### LSTM ######\n",
        "  ##################\n",
        "    \n",
        "  model_lstm.train()\n",
        "  for batch_idx, (xx, yy) in enumerate(all_data_train):\n",
        "    xx,yy = model_lstm.input_format(xx, yy)\n",
        "    pred_batch_lstm = model_lstm(xx)\n",
        "    optimizer_lstm.zero_grad()\n",
        "    loss_batch_lstm = model_lstm.criterion(pred_batch_lstm,yy)\n",
        "    loss_batch_lstm.backward()\n",
        "#     torch.nn.utils.clip_grad_norm(model_lstm.parameters(), 0.2, norm_type=2)\n",
        "    optimizer_lstm.step()\n",
        "\n",
        "    \n",
        "  model_lstm.eval()\n",
        "  lt = batch_loss(all_data_train, model_lstm)\n",
        "  lv = batch_loss(all_data_valid, model_lstm)\n",
        "  loss_train_data_lstm = np.append(loss_train_data_lstm, lt)\n",
        "  loss_valid_data_lstm = np.append(loss_valid_data_lstm, lv)\n",
        "  \n",
        "  total_norm_lstm = np.append(total_norm_lstm, model_lstm.grad_norm())  \n",
        "  \n",
        "  ##################\n",
        "  ##################\n",
        "    \n",
        "  if e_%10 == 0: \n",
        "    print('N. of Epochs # {}, RNN, Train loss = {}  ----    LSTM, Train loss = {}'\n",
        "            .format(e_,loss_train_data_rnn[e_],loss_train_data_lstm[e_]))\n",
        "    \n",
        "  lr_ = adjust_lr(optimizer_rnn,lr0, e_, number_epoch)\n",
        "  lr_ = adjust_lr(optimizer_lstm,lr0, e_, number_epoch)\n",
        "    \n",
        "  tf = time.clock()\n",
        "print('Terminé, %.1f sec'%(tf - t0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cdmCKBO9jR_d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(16,12))\n",
        "ax1.plot(loss_train_data_rnn, '-k', label ='train')\n",
        "ax1.plot(loss_valid_data_rnn, '-r', label = 'valid')\n",
        "\n",
        "ax2.plot(total_norm_rnn, '-k', label = 'norm rnn')\n",
        "\n",
        "ax3.plot(loss_train_data_lstm, '-k', label ='train')\n",
        "ax3.plot(loss_valid_data_lstm, '-r', label = 'valid')\n",
        "\n",
        "ax4.plot(total_norm_lstm, '-k', label = 'norm lstm')\n",
        "\n",
        "ax3.set_xlabel('epoch', fontsize=14)\n",
        "ax4.set_xlabel('epoch', fontsize=14)\n",
        "\n",
        "ax1.set_ylabel('MSE loss', fontsize=14)\t\n",
        "ax3.set_ylabel('MSE loss', fontsize=14)\t\n",
        "\n",
        "ax1.set_title('RNN', fontsize = 14)\n",
        "ax2.set_title('RNN', fontsize = 14)\n",
        "ax3.set_title('LSTM', fontsize = 14)\n",
        "ax4.set_title('LSTM', fontsize = 14)\n",
        "legend = ax1.legend(loc='upper right', fontsize=14)\n",
        "legend = ax3.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "\n",
        "ax1.set_xlim((0,number_epoch))\n",
        "ax1.set_ylim((0))\n",
        "ax2.set_ylim((0))\n",
        "ax3.set_xlim((0,number_epoch))\n",
        "ax3.set_ylim((0))\n",
        "ax4.set_ylim((0))\n",
        "\n",
        "xmin, xmax = ax1.get_xlim()\n",
        "ymin, ymax = ax1.get_ylim()\n",
        "\n",
        "ax1.text(0.1*(xmax - xmin) + xmin, 0.6*(ymax-ymin) + ymin,\n",
        "         'num_layers : %.f'%num_layers, fontsize=14)\n",
        "ax1.text(0.1*(xmax - xmin) + xmin, 0.52*(ymax-ymin) + ymin,\n",
        "         'hidden_size : %.f'%hidden_size, fontsize=14)\n",
        "ax1.text(0.1*(xmax - xmin) + xmin, 0.44*(ymax-ymin) + ymin,\n",
        "         'number_epoch : %.f'%number_epoch, fontsize=14)\n",
        "ax1.text(0.1*(xmax - xmin) + xmin, 0.36*(ymax-ymin) + ymin,\n",
        "         'lr$_0$ : %.1f'%lr0, fontsize=14)\n",
        "\n",
        "\n",
        "for ax in [ax1,ax2,ax3,ax4]:\n",
        "  for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n",
        "               + ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "    item.set_fontsize(14)\n",
        "\n",
        "plt.show()          \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "63mTDEWcT5ZK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nF8hxt0NugZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MLP\n",
        "\n",
        "Pour la suite, nous vous proposons d'intégrer ce jeu de données à un MLP, tel que présenté par Arsène Fansi Tchango.\n",
        "\n",
        "Voici les données : \n"
      ]
    },
    {
      "metadata": {
        "id": "e_imza-4H0oO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def linear_ini(ll,initialization):\n",
        "    '''\n",
        "    inputs : linear layer (ll) and the initialization\n",
        "    output : linear layer with the chosen initialization\n",
        "    '''\n",
        "    if initialization == 'glorot':\n",
        "        ll.weight.data = nn.init.xavier_uniform(ll.weight.data, gain=1)\n",
        "        ll.bias.data = nn.init.constant(ll.bias.data, 0)\n",
        "    return ll\n",
        "\n",
        "def prediction_batch(data_batch, model):\n",
        "  loss = 0\n",
        "  n = 0\n",
        "  for x,y in data_batch:\n",
        "    \n",
        "    out = 0\n",
        "    x,y = model.input_shape(x, y)\n",
        "    pred_batch = model(x)          \n",
        "    loss_batch = model.criterion(pred_batch, y)\n",
        "    loss += loss_batch\n",
        "    n += y.shape[0]\n",
        "    \n",
        "  return loss.data[0] / n\n",
        "  \n",
        "  \n",
        "class MLPLinear(nn.Module):\n",
        "  def __init__(self, dimensions, cuda):\n",
        "    super(MLPLinear, self).__init__()\n",
        "    self.h0 = int(dimensions[0])\n",
        "    self.h1 = int(dimensions[1])\n",
        "    self.h2 = int(dimensions[2])       \n",
        "    self.h3 = int(dimensions[3])       \n",
        "    \n",
        "    self.fc1 = torch.nn.Linear(self.h0, self.h1)\n",
        "    self.fc2 = torch.nn.Linear(self.h1, self.h2)\n",
        "    self.fc3 = torch.nn.Linear(self.h2, self.h3)\n",
        "    \n",
        "    self.relu = nn.ReLU()\n",
        "    self.criterion = nn.MSELoss()\n",
        "    self.cuda = cuda\n",
        "\n",
        "    if cuda: \n",
        "      self.fc1.cuda()\n",
        "      self.fc2.cuda()\n",
        "      self.fc3.cuda()\n",
        "      self.relu.cuda()\n",
        "      self.criterion.cuda()\n",
        "\n",
        "  def initialization(self,method):\n",
        "    self.fc1 = linear_ini(self.fc1, method)\n",
        "    self.fc2 = linear_ini(self.fc2, method)\n",
        "    self.fc3 = linear_ini(self.fc3, method)\n",
        "\n",
        "  def input_shape(self, x, y):  \n",
        "    y = y.float()\n",
        "    \n",
        "    if self.cuda : \n",
        "      x = Variable(x.cuda())\n",
        "      y = Variable(y.cuda())\n",
        "    else: \n",
        "      x = Variable(x)\n",
        "      y = Variable(y)\n",
        "    return x,y\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.fc3(out)    \n",
        "    return  out\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-5DXS_iQ3jp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "T = 100\n",
        "x,y = data_set(20000, T, [-1000,1000])\n",
        "\n",
        "x = torch.FloatTensor(x)\n",
        "y = torch.FloatTensor(y).long()\n",
        "\n",
        "xtrain, ytrain = standardized(x[:10000], y[:10000])\n",
        "xvalid, yvalid = standardized(x[10000:15000], y[10000:15000])\n",
        "xtest, ytest = standardized(x[15000:], y[15000:])\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "all_data_train = data_utils.DataLoader(data_utils.TensorDataset(xtrain, ytrain),\n",
        "                                       batch_size, shuffle=True)\n",
        "all_data_valid = data_utils.DataLoader(data_utils.TensorDataset(xvalid, yvalid), \n",
        "                                       batch_size, shuffle=False)\n",
        "all_data_test = data_utils.DataLoader(data_utils.TensorDataset(xtest, ytest),\n",
        "                                       batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m889WFzDeF-q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "cuda = True\n",
        "lr0 = 0.01\n",
        "batch_size = 100\n",
        "nb_epochs = 300\n",
        "\n",
        "# weight_decay_0 = 2.5 * batch_size / xtrain.shape[0]\t\t\n",
        "model = MLPLinear([T, 300, 200, 1], cuda) \n",
        "model.initialization('glorot')\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr0)\t\n",
        "\n",
        "loss_train = np.empty((nb_epochs)) \n",
        "loss_valid = np.empty((nb_epochs)) \n",
        "\n",
        "L2norm = np.empty((nb_epochs))\t\t\n",
        "t0 = time.clock()\n",
        "for e_ in range(nb_epochs):\n",
        "  for batch_idx, (x, y) in enumerate(all_data_train):\n",
        "    loss_batch=0\n",
        "    xt,yt = model.input_shape(x, y)\n",
        "    pred_batch = model(xt)          \n",
        "    optimizer.zero_grad()\n",
        "    loss_batch = model.criterion(pred_batch, yt)\n",
        "    loss_batch.backward()\n",
        "    loss_batch += loss_batch\n",
        "    optimizer.step()\t\t\t\t\n",
        "  loss_train[e_] = prediction_batch(all_data_train ,model)\n",
        "  loss_valid[e_] = prediction_batch(all_data_valid, model)\n",
        "  if e_%20==0: \n",
        "    print('Epoch #{}, Loss train = {}, Loss valid = {}'.format(e_, loss_train[e_], loss_valid[e_]))\n",
        "tf = time.clock()\n",
        "print('Terminé, %.1f sec'%(tf - t0))    \n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuVlmRo7eGCe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "xtest_, ytest_ = model.input_shape(xtest, ytest)\n",
        "pred_test_all = model(xtest_).view(xtest_.shape[0])\n",
        "\n",
        "L1_loss = torch.sum(torch.abs(pred_test_all - ytest_)).data[0]\n",
        "\n",
        "print('LSTM L1 loss = {}'.format(L1_loss))\n",
        "print('      ')\n",
        "print('Prediction')\n",
        "print('___________________')\n",
        "print(pred_test_all[:10])\n",
        "print('      ')\n",
        "print('Target')\n",
        "print('___________________')\n",
        "print(ytest_[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fq5govzDZI3D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}