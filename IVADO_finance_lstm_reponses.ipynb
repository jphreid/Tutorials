{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IVADO_finance_lstm_reponses.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"m0CMjiv8jx7Y","colab_type":"text"},"cell_type":"markdown","source":["<h1 align=\"center\">Atelier thématique en Finance et Assurance</h1> \n","<br/>\n","<h1 align=\"center\">Tutoriel : Tutoriel sur les réseaux neuronaux récurrents (RNN & LSTM)</h1>\n","<h2 align=\"center\">Francis Grégoire</h2>\n","<h2 align=\"center\">Jean-Philippe Reid</h2>\n","<h3 align=\"center\">MILA R&D et transfert technologique</h3>\n","\n"]},{"metadata":{"id":"-Lz8J0-xIaaR","colab_type":"text"},"cell_type":"markdown","source":["## References\n","\n","http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","\n","https://distill.pub/2016/augmented-rnns/\n","\n"]},{"metadata":{"id":"-3gKrd0pIaqk","colab_type":"text"},"cell_type":"markdown","source":["## Librairies"]},{"metadata":{"id":"Le5EZ86zO3uz","colab_type":"text"},"cell_type":"markdown","source":["**General note: this tutorial is optimized with Google Chrome.**"]},{"metadata":{"id":"udELFBoofLww","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import io\n","import os\n","import platform\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from google.colab import files\n","from matplotlib.pyplot import cm "],"execution_count":0,"outputs":[]},{"metadata":{"id":"o6R35aYPLDMZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def install_pytorch():\n","    os = platform.system()\n","    if os == \"Linux\":\n","        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n","    elif os == \"Windows\":\n","        !pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl \n","    !pip3 install torchvision\n","\n","\n","    \n","# Install PyTorch.\n","try:\n","    import torch\n","except ImportError:\n","    install_pytorch()\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KMrJS7ugInKp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def adjust_fontsize(ax):\n","    \"\"\"Set the fontsize labels of given axis to 14.\"\"\"\n","    for ax in ax:\n","        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n","                     + ax.get_xticklabels() + ax.get_yticklabels()):\n","            item.set_fontsize(14)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"1LMfzYv5JhQz","colab_type":"text"},"cell_type":"markdown","source":["## Objective\n","FG\n","\n","The objective of the tutorial is to introduce recurrent neural networks (RNNs), which are widely used deep neural networks for time series and sequential data modeling. First, we introduce all the inputs and outputs of RNNs. We will then show how to train such models in order make predictions using synthetic and real financial data. \n"]},{"metadata":{"id":"wbpV9PniFux7","colab_type":"text"},"cell_type":"markdown","source":["![alt text](https://raw.githubusercontent.com/jphreid/tutorial_ivado/master/lstm_finance.001.jpeg)\n","\n","Fig. 1"]},{"metadata":{"id":"H3rA2uLLFn7Q","colab_type":"text"},"cell_type":"markdown","source":["![alt text](https://github.com/jphreid/tutorial_ivado/raw/master/lstm_finance.002.jpeg)\n","\n","Fig. 2"]},{"metadata":{"id":"RCnneiTsqoqY","colab_type":"text"},"cell_type":"markdown","source":["## Long Short Term Memory Recurrent Neural Networks (LSTM)"]},{"metadata":{"id":"IAisYYriqoqZ","colab_type":"text"},"cell_type":"markdown","source":["### LSTM layer\n","###  __ = nn.LSTM(`input_size`, `hidden_size`, `num_layers`)\n","JPh "]},{"metadata":{"id":"u93VR8oVqoqa","colab_type":"text"},"cell_type":"markdown","source":["An LSTM layer includes numerous parameters which are constrained to the dataset dimensions, i.e. `input_size`, and also to the chosen hyperparameter values such as `hidden_size` and `num_layers`.\n","\n","https://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"]},{"metadata":{"id":"FgCALpkgqoqb","colab_type":"text"},"cell_type":"markdown","source":["### `Inputs`\n","### __ = LSTM(*input*, (h0, c0))"]},{"metadata":{"id":"EXaWaGleqoqb","colab_type":"text"},"cell_type":"markdown","source":["The inputs are: `input`, `h0` and `c0`.\n","\n","`input` represents a mini-batch. \n","\n","`h0` and `c0`  are the hidden and cell states. They are set to zero if  not defined.\n","\n","See Fig. 4.\n"]},{"metadata":{"id":"JrRFyn_Tqoqc","colab_type":"text"},"cell_type":"markdown","source":["### `Input` (data)"]},{"metadata":{"id":"RKuc_SyMqoqd","colab_type":"text"},"cell_type":"markdown","source":["`input` =  torch.Tensor(seq_len, batch_size, input_size).\n","\n","The input (data) is a tensor of dimensions `seq_len` x `batch_size` x `input_size`. \n","\n","See Fig. 4."]},{"metadata":{"id":"Dh9ca_HMiKuC","colab_type":"text"},"cell_type":"markdown","source":["### Outputs\n","### `output`, (`hn`, `cn`) = LSTM(*input*, (h0, c0))"]},{"metadata":{"id":"IZABJ8pLFPTa","colab_type":"text"},"cell_type":"markdown","source":["`output`= torch.Tensor(seq_len, batch_size, hidden_size x num_directions).\n","\n","`hn` =  `cn` = torch.Tensor(num_layers * num_directions, batch_size, hidden_size).\n","\n"," See Fig. 5."]},{"metadata":{"id":"zvtzfm1qaj92","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.003.jpeg)\n","\n","Fig. 3: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"]},{"metadata":{"id":"9P1NDwZqqoqc","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.004.jpeg)\n","\n","Fig. 4: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"]},{"metadata":{"id":"n-bi9ydcqoqq","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.005.jpeg)\n","\n","Fig.5 : http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"]},{"metadata":{"id":"2M1DS-Jddj--","colab_type":"text"},"cell_type":"markdown","source":["## Example (Synthetic sinewave data)"]},{"metadata":{"id":"R1fDGjOnoIXO","colab_type":"text"},"cell_type":"markdown","source":["### Data preprocessing \n","FG"]},{"metadata":{"id":"K7D0GuNOPI3N","colab_type":"text"},"cell_type":"markdown","source":["We load `data` into a Python list which contains a time series of length 5001. "]},{"metadata":{"id":"pZg1qVpVIqjB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"code"},"cell_type":"code","source":["# Load data from local file system.\n","file_path = \"./sinewave.csv\"\n","\n","if not os.path.isfile(file_path):\n","    uploaded = files.upload()  # Select ../../DATA/Donnees_finance/sinewave.csv.\n","\n","# Read sinewave data.\n","with open(file_path, \"r\") as f:\n","    data = [float(line) for line in f.readlines()]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LA2YOfZqIKz_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(\"Total length: {}\".format(len(data)))\n","print()\n","fig = plt.figure(figsize=(14, 5))\n","plt.plot(data, color=\"royalblue\");\n","plt.xlim((0, 5001))\n","plt.xlabel(\"$t$\")\n","\n","adjust_fontsize(fig.axes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QrRFH6Vyeo7p","colab_type":"text"},"cell_type":"markdown","source":["\n","We first divide the raw dataset into 4950 series with seq_len of 51 (with overlaps). "]},{"metadata":{"id":"bRpitZbzOonR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Prepare data.\n","seq_len = 50 + 1\n","sequences = torch.FloatTensor([data[t:t+seq_len] for t in range(len(data)-seq_len)])\n","\n","print('Dimensions: {} x {}'.format(*sequences.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5EPU6kSpQ-y4","colab_type":"text"},"cell_type":"markdown","source":["\n","We then separate the dataset in two parts, i.e. train and test datasets."]},{"metadata":{"id":"xrtKpGxNRCkc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["split_row = round(0.90*sequences.size(0))\n","x_train = sequences[:split_row, :-1]\n","y_train = sequences[:split_row, -1]\n","x_test = sequences[split_row:, :-1]\n","y_test = sequences[split_row:, -1]\n","\n","print('x train: {} x {}'.format(*x_train.size()))\n","print('y train: {} x 1'.format(y_train.size(0)))\n","print(' x test: {} x {}'.format(*x_test.size()))\n","print(' y test: {} x 1'.format(y_test.size(0)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q5fTtlcWOozr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Plot the first 4 sequences.\n","color = cm.rainbow(np.linspace(0, 1, 4))\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","\n","for i in [3, 2, 1, 0]:\n","    ax1.plot(np.arange(50)+i, x_train[i].data.numpy(), linewidth=3, color=color[i]);\n","    ax1.plot(50+i, y_train[i].data.numpy(), 'o', color=color[i]);\n","    ax1.set_xlim(0, 55)\n","    ax1.set_xlabel('$t$')\n","    \n","for i in range(4):\n","    ax2.plot(np.arange(50), x_train[i].data.numpy(), linewidth=3, color=color[i]);\n","    ax2.plot(51, y_train[i].data.numpy(), 'o', color=color[i]);\n","    ax2.set_xlim(0, 55)\n","    ax2.set_xlabel('$t-t_0$')\n","    \n","adjust_fontsize([ax1, ax2])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ona4Sr9Mfs8S","colab_type":"text"},"cell_type":"markdown","source":["Let's prepare an input data with a `batch_size` of 4, `seq_len` of 50 and `input_size` of 1.\n","\n","The  input data format has to be  (seq_len, batch_size, input_size). "]},{"metadata":{"id":"oD4HNqjcOo8G","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Select a minibatch of 4. \n","x = x_train[:4, :, None]\n","y = y_train[:4]\n","print('x - original dimension: {} x {} x {}'.format(*x.shape))\n","\n","# Reshape the tensor in the needed format (seq_len, batch_size, input_size).\n","x = x.permute(1, 0, 2)\n","print('x - new dimension: {} x {} x {}'.format(*x.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iSVAYtwAl2Sh","colab_type":"text"},"cell_type":"markdown","source":["__Q1__ : Given the example above, what are the data input parameters? \n","\n","* batch_size = \n","* seq_len = \n","* input = \n"]},{"metadata":{"id":"QbwYsapBl3_-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print('batch_size: {}'.format(x.shape[1]))\n","print('seq_len: {}'.format(x.shape[0]))\n","print('input_size: {}'.format(x.shape[2]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MAJ9tNujqoql","colab_type":"text"},"cell_type":"markdown","source":["__Q2__  : Give an example where the `input_size` is superior to one, i.e. `input_size` $>1$.\n","\n"]},{"metadata":{"id":"ynhYKA9Blv9H","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.006.jpeg)\n","\n","\n","Fig.6 : Example with `input_size` $>1$."]},{"metadata":{"id":"wAO6btw5or2Q","colab_type":"text"},"cell_type":"markdown","source":["### Create LSTM layer "]},{"metadata":{"id":"mpoFpnjrmNlQ","colab_type":"text"},"cell_type":"markdown","source":["__Q3__  : Creat an LSTM layer. See Fig.3 above. This [example](https://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM) might be useful. \n","\n","Take the time to understand the dimension of the outputs. See Fig.5 for more information. "]},{"metadata":{"id":"WFYNzEKbOo_-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Q3\n","# Parameters. \n","input_size = 1\n","hidden_size = 12\n","num_layers = 1\n","\n","# LSTM layer.\n","lstm = nn.LSTM(input_size, hidden_size, num_layers)\n","\n","# Forward pass.\n","output_0, (hn, cn) = lstm(x)\n","\n","print('Dimension - output_0: {}'.format(output_0.shape))\n","print('Dimension - hn: {}'.format(hn.shape))\n","print('Dimension - cn: {}'.format(cn.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MX0t3nT98kAn","colab_type":"text"},"cell_type":"markdown","source":["### Reorganize the dimensions of `output_0`"]},{"metadata":{"id":"ZIVvInzN8zqm","colab_type":"text"},"cell_type":"markdown","source":["Only the last component of the `output_0` tensor will be considered to make the prediction (See Fig.2).\n","\n","For example, let's consider the 3rd sequence and its associated target. "]},{"metadata":{"id":"w50E_nbBJ5-e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["fig = plt.figure(figsize=(7, 5))\n","# Third sequence.\n","plt.plot(np.arange(50), x[:, 3, 0].data.numpy(), color=\"royalblue\");\n","# Third target.\n","plt.plot(51, y[3].data.numpy(), 'o',  color=\"royalblue\");\n","\n","adjust_fontsize(fig.axes)\n","plt.xlim(0, 52)\n","plt.xlabel('t')\n","plt.legend(('Third sequence', 'Third target'))\n","plt.title('Third sequence and target')\n","\n","adjust_fontsize(fig.axes)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZO3jVi_zsJH0","colab_type":"text"},"cell_type":"markdown","source":["The last component of  `output_0` is given by:"]},{"metadata":{"id":"S8pvmb6hqStL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(output_0[-1, 3, :])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sDD1ICcJL_B7","colab_type":"text"},"cell_type":"markdown","source":["Note that the tensor `hn` contains the last hidden state of  each sequence for each layer. In principle, we could also use `hn` instead of `output_0`."]},{"metadata":{"id":"7rh_45NWMbqT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(hn[-1, 3, :])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ewppj5yvsVJT","colab_type":"text"},"cell_type":"markdown","source":["__Q4__  : Propose a strategy to transform the tensor `output_0` of dimension $\\left[50, 4, 12\\right]$ to a tensor of dimension $\\left[ 4, 1\\right]$, i.e. same dimension as the targets. Specifically, what mathematical operation should be used to complete this task? \n","\n","Answer : Matrix product between `output_0` and a matrix $\\mathbf{M}$. In this case, the dimension of $\\mathbf{M}$ is `hidden_size` $\\times 1$. \n","\n","\\begin{align}\n","\\text{output} =  \\text{output_0}\\;  \\mathbf{M}\n","\\end{align}\n","\n","*  You can use the function `torch.matmul()`. You need to generate a random tensor with the proper dimension that multiplies `output_0`. See the documentation of:  [torch.Tensor(.)](https://pytorch.org/docs/master/tensors.html#torch.Tensor) and [torch.matmul(.)](https://pytorch.org/docs/master/torch.html?highlight=matmul#torch.matmul).\n","\n","\n","* You can also use a linear layer  `nn.Linear()` matching the dimension of $\\mathbf{M}$. See the documentation of [nn.Linear(.)](https://pytorch.org/docs/master/nn.html#linear).\n","\n"]},{"metadata":{"id":"BaS5Ku1GJ6Dr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# With torch.Tensor() and torch.matmul(). \n","M = torch.Tensor(hidden_size, input_size)\n","output = torch.matmul(output_0[-1, :, :], M)\n","\n","print('First technique: {} x {}'.format(*output.shape))\n","\n","# With nn.Linear().\n","LL = nn.Linear(hidden_size, input_size)\n","output = LL(output_0[-1, :, :])\n","\n","print('Second technique: {} x {}'.format(*output.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sk1hRPDdykBQ","colab_type":"text"},"cell_type":"markdown","source":["## Our first LSTM"]},{"metadata":{"id":"VjkEMrMsF1ey","colab_type":"text"},"cell_type":"markdown","source":["### The model class"]},{"metadata":{"id":"Nq3GMYv0GDl7","colab_type":"text"},"cell_type":"markdown","source":["__Q5__  : Read through the class LSTM() and complete the function LSTM.forward().\n","\n","\n","\n"]},{"metadata":{"id":"v28W5x5nIzyH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class LSTM(nn.Module):    \n","    def __init__(self, input_size, hidden_size, \n","                 num_layers=1, dropout=0, bidirectional=False):\n","        \"\"\"\n","        Args:\n","          input_size: number of features in the input of the LSTM.\n","          hidden_size: number of features in the hidden state.\n","          num_layers: number of recurrent layers (default: 1).\n","          dropout: if non-zero, introduces a Dropout layer on the outputs of each\n","                   LSTM layer except the last layer, with dropout probability equal\n","                   to dropout (default: 0).\n","          bidirectional: if True, becomes a bidirectional LSTM (default: False).\n","        \"\"\"\n","        super(LSTM, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","\n","        self.lstm = nn.LSTM(input_size,\n","                            hidden_size,\n","                            num_layers,\n","                            dropout=dropout,\n","                            bidirectional=bidirectional)\n","        self.linear = nn.Linear(hidden_size, 1)\n","        \n","    def forward(self, inputs, hidden):\n","        \"\"\"\n","        Args:\n","          inputs: tensor containing the features of the input sequence, (seq_len, batch_size, input_size).\n","          hidden: previous hidden state and cell state for each element in the batch,\n","                  ((num_layers*num_directions, batch_size, hidden_size),\n","                   (num_layers*num_directions, batch_size, hidden_size)).\n","                   \n","        Returns:\n","          predictions: one-step ahead predictions, (batch_size, 1).\n","          outputs: output features h_t of the last layer of the LSTM, (seq_len, batch_size, hidden_size).\n","          hidden: hidden state h_t and cell state c_t for t=1,\n","                  ((num_layers*num_directions, batch_size, hidden_size),\n","                   (num_layers*num_directions, batch_size, hidden_size)).\n","        \"\"\"\n","        \n","        # Q5: use self.lstm and self.linear with the inputs and hidden arguments.\n","        outputs, hidden = self.lstm(inputs, hidden)\n","        predictions = self.linear(outputs[-1])\n","        return predictions.squeeze(1), outputs, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        \"\"\"Initializes the hidden state and cell state of the LSTM to default zero values.\n","        \n","        Args:\n","          batch_size: batch size at each time step.\n","          \n","        Returns:\n","          hidden: hidden state h_t and cell state c_t at t=0, \n","                  ((num_layers*num_directions, batch_size, hidden_size),\n","                   (num_layers*num_directions, batch_size, hidden_size)).\n","        \"\"\"\n","        num_directions = 2 if self.bidirectional else 1\n","        hidden = (torch.zeros(self.num_layers*num_directions, batch_size, self.hidden_size),\n","                  torch.zeros(self.num_layers*num_directions, batch_size, self.hidden_size))\n","        return hidden\n","    \n","    \n","def get_batch(x, y, i, batch_size):\n","    \"\"\"Generate batch data for x and y.\"\"\"\n","    if x.dim() == 2:\n","        x = x.unsqueeze(2)\n","    batch_x = x[(i*batch_size):(i*batch_size)+batch_size, :, :]\n","    batch_y = y[(i*batch_size):(i*batch_size)+batch_size]\n","\n","    # Reshape Tensors into (seq_len, batch_size, input_size) format for the LSTM.\n","    batch_x = batch_x.transpose(0, 1)\n","    \n","    return batch_x, batch_y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CGhV-zgZuZ0v","colab_type":"text"},"cell_type":"markdown","source":["### Model training"]},{"metadata":{"id":"HxspWwpuo76j","colab_type":"text"},"cell_type":"markdown","source":["__Q6__: Complete the training loop below. \n","\n","__Q7__: Train the model over 3 and 15 epochs and run the following forecasting cells. You may also test `hidden_size` equals to 1 and 24. Try with a `learning_rate` set to 0.02."]},{"metadata":{"id":"bDPD2BSRJp6p","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Build model.\n","input_size = 1\n","hidden_size = 24 #1\n","num_layers = 1\n","lstm = LSTM(input_size, hidden_size, num_layers)\n","\n","# Optimizer and loss function.\n","learning_rate = 0.0002 #0.02\n","max_grad_norm = 5\n","loss_fn = nn.MSELoss()\n","optimizer = optim.Adam(lstm.parameters(), lr=learning_rate)\n","\n","# Train model.\n","batch_size = 12\n","num_epochs = 15 #3\n","num_sequences = x_train.size(0)\n","num_batches = num_sequences // batch_size\n","print(\"Training model for {} epoch of {} batches\".format(num_epochs, num_batches))\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","\n","    # Shuffle input and target sequences.\n","    idx = torch.randperm(x_train.size(0))\n","    x = x_train[idx]\n","    y = y_train[idx]\n","\n","    for i in range(num_batches):\n","        # Get input and target batches and reshape for LSTM.\n","        batch_x, batch_y = get_batch(x_train, y_train, i, batch_size)\n","\n","        # Reset the gradient.\n","        lstm.zero_grad()\n","        \n","        # Initialize the hidden states (see the function lstm.init_hidden(batch_size)).\n","        hidden = lstm.init_hidden(batch_size)\n","        \n","        # Complete a forward pass.\n","        y_pred, outputs, hidden = lstm(batch_x, hidden)\n","        \n","        # Calculate the loss with the 'loss_fn'.\n","        loss = loss_fn(y_pred, batch_y)\n","        \n","        # Compute the gradient.\n","        loss.backward()\n","        \n","        # Clip to the gradient to avoid exploding gradient.\n","        nn.utils.clip_grad_norm_(lstm.parameters(), max_grad_norm)\n","\n","        # Make one step with optimizer.\n","        optimizer.step()\n","        \n","        # Accumulate the total loss.\n","        total_loss += loss.data\n","\n","    print(\"Epoch {}: Loss = {:.8f}\".format(epoch+1, total_loss/num_batches))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0FXn3H8utciA","colab_type":"text"},"cell_type":"markdown","source":["__Q9__ : Read carefully the `predict_one_step()` function below. Explain to your partner the operations in the for loop."]},{"metadata":{"id":"B8sQIQrkJsRx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def predict_one_step(model, data, input_size, num_steps):\n","    \"\"\"Predicts one-step ahead for each sequence of the test set.\"\"\"\n","    predictions = torch.zeros(num_steps)\n","    for i, x in enumerate(data):\n","        hidden = model.init_hidden(1)\n","        y_pred, _, _ = model(x.contiguous().view(-1, 1, input_size), hidden)\n","        predictions[i] = y_pred\n","    return predictions\n"," \n","\n","one_step_predictions = predict_one_step(lstm, x_test, input_size, y_test.size(0))\n","\n","fig = plt.figure(figsize=(14, 7))\n","plt.plot(y_test.data.numpy(), color=\"darkcyan\", label=\"True\")\n","plt.scatter(range(0, y_test.size(0)), one_step_predictions.data.numpy(), facecolors=\"none\",\n","            edgecolors=\"tomato\", linewidths=1.5, label=\"Predictions\")\n","plt.legend(loc=\"upper right\", fontsize=14)\n","adjust_fontsize(fig.axes)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ftXJ5R4_t_13","colab_type":"text"},"cell_type":"markdown","source":["__Q10__ : Read carefully the `predict_full_sequence()` function below. Explain to your partner the operations in the for loop. "]},{"metadata":{"id":"QwZSLCHrJwMk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def predict_full_sequence(model, x, input_size, num_steps):\n","    \"\"\"Predicts one-step ahead with a single sequence from the test set\n","       and use predictions to predict the remaining test set sequence\n","       for num_steps.\n","    \"\"\"\n","    predictions = torch.zeros(num_steps)\n","    hidden = model.init_hidden(1)\n","    y_pred, _, hidden = model(x.contiguous().view(-1, 1, input_size), hidden)\n","    x = torch.cat((x, y_pred))\n","    predictions[0] = y_pred\n","    for i in range(1, num_steps):\n","        y_pred, _, hidden = model(x.contiguous().view(-1, 1, input_size), hidden)\n","        x = torch.cat((x, y_pred))\n","        predictions[i] = y_pred\n","    return predictions\n","\n","\n","x = torch.FloatTensor(x_test[0])\n","num_steps = 150  # Do not set to higher value due to memory constraint\n","full_predictions = predict_full_sequence(lstm, x, input_size, num_steps)\n","fig = plt.figure(figsize=(14, 7))\n","plt.plot(range(0, x.size(0)), x.data.numpy(), color=\"royalblue\", label=\"Inputs\")\n","plt.plot(range(x.size(0), x.size(0)+num_steps), y_test[:num_steps].data.numpy(), color=\"darkcyan\", label=\"True\")\n","plt.scatter(range(x.size(0), x.size(0)+num_steps), full_predictions.data.numpy(),\n","            facecolors=\"none\", edgecolors=\"tomato\", linewidths=1.5, label=\"Predictions\")\n","plt.legend(loc=\"upper right\", fontsize=14)\n","adjust_fontsize(fig.axes)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DLNIEicUMW__","colab_type":"text"},"cell_type":"markdown","source":["## Example (Financial market data)"]},{"metadata":{"id":"ZDSAz7rZuHFG","colab_type":"text"},"cell_type":"markdown","source":["We will now use the very same model defined above, but on financial data. The dataset consists of order book data of three Energy Canadian stocks listed on the Toronto Stock Exchange (Encana, Suncor and Crescent Point Energy). The frequency of the observations is 5 seconds."]},{"metadata":{"id":"oUNzyJbaTwVW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","\n","\n","# Load data from local file system.\n","uploaded = files.upload()  # Select ~/DATA/Donnees_finance/stylizedFacts5sec.csv.\n","\n","\n","# Read financial data.\n","filename = list(uploaded.keys())[0]\n","df = pd.read_csv(io.StringIO(uploaded[filename].decode('utf-8')))\n","df.drop(df.columns.values[0], axis=1, inplace=True)\n","df.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6PjO6a522qJS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Work with a single stock.\n","stock = df.loc[df[\"symbol\"] == 1].copy()\n","\n","# Remove rows with null or NaN values.\n","stock.replace([\"null\", \"nan\", \"NaN\"], np.nan, inplace=True)\n","stock.dropna(axis=0, inplace=True)\n","\n","# Remove the symbol, date and tInt columns and rows where the average price is equal to 0.\n","stock.drop(stock.columns[[0, 1, 2]], axis=1, inplace=True)\n","stock.drop(stock[stock.avgP == 0].index, inplace=True)\n","stock.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6vR1RlYwueMI","colab_type":"text"},"cell_type":"markdown","source":["### Data preprocessing "]},{"metadata":{"id":"3L64TZgSpJoz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["split_row = round(0.90*stock.shape[0])\n","\n","# Scale data between (-1.0, 1.0).\n","feature_range = (-1, 1)\n","scaler = MinMaxScaler(feature_range)\n","scaler = scaler.fit(stock.loc[:split_row-1])\n","scaled_stock = scaler.transform(stock)\n","\n","# Split data into training and test sets.\n","train_data = scaled_stock[:split_row]\n","test_data = scaled_stock[split_row:]\n","\n","# Split data sets into inputs and outputs (we want to predict the one-step ahead average price).\n","train_in, train_out = train_data[:-1, :], train_data[1:, 0]\n","test_in, test_out = test_data[:-1, :], test_data[1:, 0]\n","\n","print('x train: {} x {}'.format(*train_in.shape))\n","print('y train: {} x 1'.format(*train_out.shape))\n","print(' x test: {} x {}'.format(*test_in.shape))\n","print(' y test: {} x 1'.format(*test_out.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5vDH2cIixyBq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Prepare sequences.\n","seq_len = 50\n","\n","x_train = torch.FloatTensor([train_in[t:t+seq_len] for t in range(len(train_in)-seq_len+1)])\n","y_train = torch.FloatTensor(train_out[seq_len-1:])\n","\n","x_test = torch.FloatTensor([test_in[t:t+seq_len] for t in range(len(test_in)-seq_len+1)])\n","y_test = torch.FloatTensor(test_out[seq_len-1:])\n","\n","fig1 = plt.figure(figsize=(16, 4))\n","plt.plot(train_in[:, 0], color=\"royalblue\", label=\"Complete sequence\")\n","plt.xlabel('$t$')\n","plt.ylabel('Average price')\n","\n","fig2 = plt.figure(figsize=(8, 4))\n","plt.plot(x_train[0, :, 0].numpy(), color=\"royalblue\", label=\"First sequence\")\n","plt.xlabel('$t$')\n","plt.ylabel('Average price')\n","\n","adjust_fontsize(fig1.axes)\n","adjust_fontsize(fig2.axes)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qlD7TrxsyMc5","colab_type":"text"},"cell_type":"markdown","source":["### Model training"]},{"metadata":{"id":"HeqsJ8MDrbXh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Build model.\n","input_size = x_train.size(2)\n","hidden_size = 48\n","num_layers = 1\n","lstm = LSTM(input_size, hidden_size, num_layers)\n","\n","# Optimizer and loss function.\n","learning_rate = 0.0002\n","max_grad_norm = 5\n","loss_fn = nn.MSELoss()\n","optimizer = optim.Adam(lstm.parameters(), lr=learning_rate)\n","\n","# Train model.\n","batch_size = 128\n","num_epochs = 8\n","num_sequences = x_train.size(0)\n","num_batches = num_sequences // batch_size\n","print(\"Training model for {} epoch of {} batches\".format(num_epochs, num_batches))\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","\n","    # Shuffle input and target sequences.\n","    idx = torch.randperm(x_train.size(0))\n","    x = x_train[idx]\n","    y = y_train[idx]\n","\n","    for i in range(num_batches):\n","        # Get input and target batches and reshape for LSTM.\n","        batch_x, batch_y = get_batch(x_train, y_train, i, batch_size)\n","\n","        # Reset the gradient.\n","        lstm.zero_grad()\n","        \n","        # Initialize the hidden states. \n","        hidden = lstm.init_hidden(batch_size)\n","        \n","        # Complete a forward pass.\n","        y_pred, outputs, hidden = lstm(batch_x, hidden)\n","        \n","        # Calculate the loss with the 'loss_fn'.\n","        loss = loss_fn(y_pred, batch_y)\n","        \n","        # Compute the gradient.\n","        loss.backward()\n","        \n","        # Clip the gradient to avoid exploding gradient.\n","        nn.utils.clip_grad_norm_(lstm.parameters(), max_grad_norm)\n","        \n","        # Make one step with the optimizer.\n","        optimizer.step()\n","        \n","        # Accumulate the total loss.\n","        total_loss += loss.data\n","\n","    print(\"Epoch {}: Loss = {:.8f}\".format(epoch+1, total_loss/num_batches))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kZl_sVR6NQYy","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["one_step_predictions = predict_one_step(lstm, x_test, input_size, y_test.size(0))\n","\n","fig = plt.figure(figsize=(14, 10))\n","plt.plot(y_test.data.numpy(), color=\"darkcyan\", label=\"True\")\n","plt.plot(one_step_predictions.data.numpy(), color=\"tomato\", label=\"Predictions\")\n","plt.legend(loc=\"upper right\")\n","adjust_fontsize(fig.axes, fontsize=14)\n","plt.show()"],"execution_count":0,"outputs":[]}]}